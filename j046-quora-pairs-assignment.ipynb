{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6277,"databundleVersionId":323734,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-15T05:45:08.086716Z","iopub.execute_input":"2024-09-15T05:45:08.087092Z","iopub.status.idle":"2024-09-15T05:45:08.095395Z","shell.execute_reply.started":"2024-09-15T05:45:08.087055Z","shell.execute_reply":"2024-09-15T05:45:08.094277Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/quora-question-pairs/train.csv.zip\n/kaggle/input/quora-question-pairs/sample_submission.csv.zip\n/kaggle/input/quora-question-pairs/test.csv\n/kaggle/input/quora-question-pairs/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q sentence-transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T05:45:11.730169Z","iopub.execute_input":"2024-09-15T05:45:11.731043Z","iopub.status.idle":"2024-09-15T05:45:24.834015Z","shell.execute_reply.started":"2024-09-15T05:45:11.730999Z","shell.execute_reply":"2024-09-15T05:45:24.832738Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!pip install -q transformers datasets\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T05:45:24.836427Z","iopub.execute_input":"2024-09-15T05:45:24.836969Z","iopub.status.idle":"2024-09-15T05:45:37.910745Z","shell.execute_reply.started":"2024-09-15T05:45:24.836921Z","shell.execute_reply":"2024-09-15T05:45:37.909730Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(train_data.columns)\nprint(test_data.columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T05:45:43.612018Z","iopub.execute_input":"2024-09-15T05:45:43.612968Z","iopub.status.idle":"2024-09-15T05:45:43.643514Z","shell.execute_reply.started":"2024-09-15T05:45:43.612927Z","shell.execute_reply":"2024-09-15T05:45:43.642221Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_data\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_data\u001b[38;5;241m.\u001b[39mcolumns)\n","\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"],"ename":"NameError","evalue":"name 'train_data' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Now you can import and use the libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import f1_score\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom sentence_transformers import CrossEncoder\nimport re\n\n# Load data\ntrain_path = '/kaggle/input/quora-question-pairs/train.csv.zip'\ntest_path = '/kaggle/input/quora-question-pairs/test.csv.zip'\n\ntrain_data = pd.read_csv(train_path, low_memory=False)\ntest_data = pd.read_csv(test_path, low_memory=False)\n\n# Preprocessing function\ndef preprocess(text):\n    if isinstance(text, str):\n        text = text.lower()\n        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n    return \"\"\n\n# Apply preprocessing to the data\ntrain_data['question1'] = train_data['question1'].apply(preprocess)\ntrain_data['question2'] = train_data['question2'].apply(preprocess)\ntest_data['question1'] = test_data['question1'].apply(preprocess)\ntest_data['question2'] = test_data['question2'].apply(preprocess)\n\n# Convert data into InputExample format for training\ntrain_examples = [InputExample(texts=[row['question1'], row['question2']], label=float(row['is_duplicate'])) for _, row in train_data.iterrows()]\n\n# For the test dataset, we don't have labels, so we skip adding the label\ntest_examples = [InputExample(texts=[row['question1'], row['question2']]) for _, row in test_data.iterrows()]\n\n# Create DataLoader\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n\n# Bi-Encoder Evaluation Function (For validation)\ndef evaluate_bi_encoder(model, test_examples):\n    predictions = []\n    for example in test_examples:\n        embeddings1 = model.encode(example.texts[0], convert_to_tensor=True)\n        embeddings2 = model.encode(example.texts[1], convert_to_tensor=True)\n        cosine_sim = torch.nn.functional.cosine_similarity(embeddings1.unsqueeze(0), embeddings2.unsqueeze(0)).item()\n        predictions.append(1 if cosine_sim > 0.5 else 0)\n    return predictions\n\n# Train a Bi-Encoder with Cosine Similarity Loss\ndef train_bi_encoder_with_cosine_loss():\n    model_cosine = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n    loss_cosine = losses.CosineSimilarityLoss(model=model_cosine)\n    model_cosine.fit(train_objectives=[(train_dataloader, loss_cosine)], epochs=3, warmup_steps=100)\n    print(\"Finished training with Cosine Loss.\")\n    return model_cosine\n\n# Train a Bi-Encoder with Contrastive Loss\ndef train_bi_encoder_with_contrastive_loss():\n    model_contrastive = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n    loss_contrastive = losses.ContrastiveLoss(model=model_contrastive)\n    model_contrastive.fit(train_objectives=[(train_dataloader, loss_contrastive)], epochs=3, warmup_steps=100)\n    print(\"Finished training with Contrastive Loss.\")\n    return model_contrastive\n\n# Train a Bi-Encoder with Multiple Negatives Ranking Loss\ndef train_bi_encoder_with_mnr_loss():\n    model_mnr = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n    loss_mnr = losses.MultipleNegativesRankingLoss(model=model_mnr)\n    model_mnr.fit(train_objectives=[(train_dataloader, loss_mnr)], epochs=3, warmup_steps=100)\n    print(\"Finished training with MNR Loss.\")\n    return model_mnr\n\n# Train a Cross-Encoder\ndef train_cross_encoder():\n    # Convert test data for inference\n    test_pairs = [[row['question1'], row['question2']] for _, row in test_data.iterrows()]\n\n    # Initialize and train Cross-Encoder\n    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n    cross_encoder.fit(train_dataloader, epochs=3)\n\n    # Evaluation function for Cross-Encoder\n    predictions = cross_encoder.predict(test_pairs)\n    print(f\"Predictions for cross-encoder: {predictions}\")\n\n# Execute the training\nmodel_cosine = train_bi_encoder_with_cosine_loss()\nmodel_contrastive = train_bi_encoder_with_contrastive_loss()\nmodel_mnr = train_bi_encoder_with_mnr_loss()\ntrain_cross_encoder()\n\n# Evaluate the models on test data\ncosine_predictions = evaluate_bi_encoder(model_cosine, test_examples)\ncontrastive_predictions = evaluate_bi_encoder(model_contrastive, test_examples)\nmnr_predictions = evaluate_bi_encoder(model_mnr, test_examples)\n\n# Print the first 10 predictions for each model\nprint(\"Cosine Similarity Predictions:\", cosine_predictions[:10])\nprint(\"Contrastive Loss Predictions:\", contrastive_predictions[:10])\nprint(\"MNR Predictions:\", mnr_predictions[:10])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T05:45:56.600782Z","iopub.execute_input":"2024-09-15T05:45:56.601601Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"053eca7ab21b49b8a3a7ae2b8c905142"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9927c045126d4e1c8bce6a1fd3ef0a90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d26bac7bde84290844d94be93bb39f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56099192533b4ab2b5db4f341caa9e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7482473c50346f8ba5361723a08b78b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"485d48fc421c4529a7e7e8d99c7a42e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f414868448f481d869ad9bea9d5ae5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e97357c1dac44c70b33962ef569313ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a030ce69e9e4106bbc79b304be68661"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91d9ed4f26894b2ebde4812b542999e9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c203da692be4bfd88308a1d10479b40"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240915_055225-bll6ro5a</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hishreyash-n-svkm-s-narsee-monjee-institute-of-managemen/sentence-transformers/runs/bll6ro5a' target=\"_blank\">checkpoints/model</a></strong> to <a href='https://wandb.ai/hishreyash-n-svkm-s-narsee-monjee-institute-of-managemen/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hishreyash-n-svkm-s-narsee-monjee-institute-of-managemen/sentence-transformers' target=\"_blank\">https://wandb.ai/hishreyash-n-svkm-s-narsee-monjee-institute-of-managemen/sentence-transformers</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hishreyash-n-svkm-s-narsee-monjee-institute-of-managemen/sentence-transformers/runs/bll6ro5a' target=\"_blank\">https://wandb.ai/hishreyash-n-svkm-s-narsee-monjee-institute-of-managemen/sentence-transformers/runs/bll6ro5a</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='37905' max='37905' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [37905/37905 1:05:10, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.167100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.148400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.139800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.136800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.134800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.134000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.131700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.131100</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.127100</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.126400</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.126500</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.125900</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.125000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.122500</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.121500</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.123200</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.121500</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.122800</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.121300</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.118600</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.120200</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.119400</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.117400</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.115800</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.119000</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.114700</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.112700</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.110900</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.110300</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.107400</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.109000</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.108200</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.109700</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.106800</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.106000</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.105600</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.106700</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.107400</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.106400</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.103600</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.103800</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.105700</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.105800</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.104300</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.104300</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.104500</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.102000</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.102700</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.102300</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.104300</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.102500</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.097700</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.096900</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.097900</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.094500</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.098300</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.096400</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.096700</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.096200</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>0.095100</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>0.096400</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>0.096200</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>0.095200</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>0.091800</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>0.093200</td>\n    </tr>\n    <tr>\n      <td>33500</td>\n      <td>0.093800</td>\n    </tr>\n    <tr>\n      <td>34000</td>\n      <td>0.095500</td>\n    </tr>\n    <tr>\n      <td>34500</td>\n      <td>0.095400</td>\n    </tr>\n    <tr>\n      <td>35000</td>\n      <td>0.094300</td>\n    </tr>\n    <tr>\n      <td>35500</td>\n      <td>0.094000</td>\n    </tr>\n    <tr>\n      <td>36000</td>\n      <td>0.094700</td>\n    </tr>\n    <tr>\n      <td>36500</td>\n      <td>0.093700</td>\n    </tr>\n    <tr>\n      <td>37000</td>\n      <td>0.092200</td>\n    </tr>\n    <tr>\n      <td>37500</td>\n      <td>0.094600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Finished training with Cosine Loss.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='37905' max='37905' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [37905/37905 1:05:48, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.016900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.014600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.014100</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.014000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.013100</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.013100</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.013000</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.012900</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.012700</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.012900</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.012500</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.012700</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.012600</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.012100</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.011700</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.011300</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.011200</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.010900</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.011200</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.011300</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.011300</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.011100</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.011200</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.010900</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.010900</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.010700</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.010900</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.010500</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.010100</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.010200</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.010000</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.010000</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>0.010000</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>33500</td>\n      <td>0.010200</td>\n    </tr>\n    <tr>\n      <td>34000</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>34500</td>\n      <td>0.009500</td>\n    </tr>\n    <tr>\n      <td>35000</td>\n      <td>0.009600</td>\n    </tr>\n    <tr>\n      <td>35500</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>36000</td>\n      <td>0.009400</td>\n    </tr>\n    <tr>\n      <td>36500</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>37000</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>37500</td>\n      <td>0.009600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Finished training with Contrastive Loss.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='37905' max='37905' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [37905/37905 1:05:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.447100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.444300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.416300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.422700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.415000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.398300</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.405800</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.417500</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.410200</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.402300</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.416500</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.389900</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.402400</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.389700</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.399700</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.397500</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.406700</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.389100</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.418700</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.401600</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.392200</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.391900</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.378600</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.394000</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.371200</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.358600</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.344200</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.338300</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.336800</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.324500</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.322900</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.331500</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.332100</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.328000</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.334500</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.347700</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.323900</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.324900</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.316000</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.331000</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.325500</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.330900</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.338300</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.342100</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.339500</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.333800</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.325200</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.324300</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.323600</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.321500</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.310800</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.287700</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.287200</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.284600</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.269900</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.274500</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.272000</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.283700</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.286200</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.283400</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>0.289700</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>0.273300</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>0.281000</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>0.272000</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>0.277400</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>0.277000</td>\n    </tr>\n    <tr>\n      <td>33500</td>\n      <td>0.284200</td>\n    </tr>\n    <tr>\n      <td>34000</td>\n      <td>0.291000</td>\n    </tr>\n    <tr>\n      <td>34500</td>\n      <td>0.282600</td>\n    </tr>\n    <tr>\n      <td>35000</td>\n      <td>0.300500</td>\n    </tr>\n    <tr>\n      <td>35500</td>\n      <td>0.287000</td>\n    </tr>\n    <tr>\n      <td>36000</td>\n      <td>0.280100</td>\n    </tr>\n    <tr>\n      <td>36500</td>\n      <td>0.276400</td>\n    </tr>\n    <tr>\n      <td>37000</td>\n      <td>0.279800</td>\n    </tr>\n    <tr>\n      <td>37500</td>\n      <td>0.283300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Finished training with MNR Loss.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b257d73566f4b0885e317aa561059a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ee654381c24dafb19d6ba8695a4b1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0fb8285c226488ab85b5f823d53a806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a07243324644429a8670cedae8676d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94c6236049864fb6927f86f77fd6b555"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a7316331e94c0d96aee9e93287ff8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/25269 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647e2c85a09b4028b5a1001ba1f8fd7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/25269 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c5290037c9a48d1876465f347e62160"}},"metadata":{}}]}]}